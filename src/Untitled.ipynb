{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1219  0.0607 -0.0159 -0.0319 -0.1434 -0.1471\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.1175  0.1372  0.3543  0.1086  0.0334  0.0777\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.1036  0.0893  0.2950  0.0383 -0.0607 -0.0566\n",
      "\n",
      "(3 ,.,.) = \n",
      "  0.0299 -0.0283  0.2418 -0.1005 -0.1825 -0.2415\n",
      "\n",
      "(4 ,.,.) = \n",
      " -0.0033 -0.1468  0.1634 -0.1739 -0.3147 -0.1039\n",
      "[torch.FloatTensor of size 5x1x6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "lstm = nn.LSTM(3, 3, bidirectional=True)  # Input dim is 3, output dim is 3\n",
    "# inputs = [autograd.Variable(torch.randn((1, 3)))\n",
    "#           for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# # initialize the hidden state.\n",
    "# hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn(1,1,3)))\n",
    "\n",
    "# for i in inputs:\n",
    "#     # Step through the sequence one element at a time.\n",
    "#     # after each step, hidden contains the hidden state.\n",
    "#     print(i)\n",
    "#     print(i.view(1,1,-1))\n",
    "#     out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "#     print(out)\n",
    "    \n",
    "inputs = autograd.Variable(torch.randn(5,1,3))\n",
    "hidden = (autograd.Variable(torch.randn(2, 1, 3)), autograd.Variable(torch.randn(2,1,3)))\n",
    "\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.0311 -0.1047 -0.0885  0.0646  0.1810  0.1586 -0.0152 -0.0785  0.2182  0.1042\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.0028  0.3119  0.0953 -0.1830  0.0750  0.1165 -0.1690 -0.1141 -0.3218 -0.2527\n",
      "\n",
      "Columns 20 to 28 \n",
      " 0.2681 -0.0648 -0.0774 -0.0386 -0.0223  0.0348 -0.0328 -0.1720  0.0232\n",
      "[torch.FloatTensor of size 1x29]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        h = self.linear1(torch.sigmoid(x)).clamp(min=0)\n",
    "        y_pred = self.linear2(h)\n",
    "        return y_pred\n",
    "    \n",
    "MLP_head = MLP(12, 120, 29)\n",
    "\n",
    "inp = Variable(torch.randn(1,12))\n",
    "print(inp.size())\n",
    "\n",
    "out = MLP_head(inp)\n",
    "\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
